<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AWS on Data Science | DSChloe</title>
    <link>https://dschloe.github.io/aws/</link>
    <description>Recent content in AWS on Data Science | DSChloe</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 May 2022 10:13:30 +0900</lastBuildDate><atom:link href="https://dschloe.github.io/aws/rss.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>S3 with Python Basic Tutorial</title>
      <link>https://dschloe.github.io/aws/04_s3/s3_basic/</link>
      <pubDate>Mon, 30 May 2022 10:13:30 +0900</pubDate>
      
      <guid>https://dschloe.github.io/aws/04_s3/s3_basic/</guid>
      <description>Bucket 만들기 Bucket을 만들어보도록 한다. import boto3 print(boto3.__version__) 1.23.5 bucket = boto3.resource(&amp;#39;s3&amp;#39;) response = bucket.create_bucket( Bucket = &amp;#34;your_bucket_name&amp;#34;, ACL=&amp;#34;private&amp;#34;, # public-read CreateBucketConfiguration = { &amp;#39;LocationConstraint&amp;#39; : &amp;#39;ap-northeast-2&amp;#39; } ) print(response) s3.Bucket(name=&#39;your_bucket_name&#39;) 버킷 대시보드에서 실제 Bucket이 만들어졌는지 확인한다. Client Bucket 이번에는 client 버킷을 생성한다. client = boto3.client(&amp;#39;s3&amp;#39;) response = client.create_bucket( Bucket = &amp;#34;your_bucket_name&amp;#34;, ACL = &amp;#34;private&amp;#34;, CreateBucketConfiguration = { &amp;#39;LocationConstraint&amp;#39; : &amp;#39;ap-northeast-2&amp;#39; } ) print(response) {&#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;1X0BAXRG653Q7Y61&#39;, &#39;HostId&#39;: &#39;WwKyxNBcd1V9x6D/WZn8twMKSWKBnkwVCPWtvarZvyNSSvqr7Q77J6OFAdWuYAwiv/nQfXoW/0U=&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;x-amz-id-2&#39;: &#39;WwKyxNBcd1V9x6D/WZn8twMKSWKBnkwVCPWtvarZvyNSSvqr7Q77J6OFAdWuYAwiv/nQfXoW/0U=&#39;, &#39;x-amz-request-id&#39;: &#39;1X0BAXRG653Q7Y61&#39;, &#39;date&#39;: &#39;Wed, 25 May 2022 03:16:52 GMT&#39;, &#39;location&#39;: &#39;http://your_bucket_name.</description>
    </item>
    
    <item>
      <title>IAM User Practice</title>
      <link>https://dschloe.github.io/aws/01_settings/iam_users/</link>
      <pubDate>Sat, 28 May 2022 10:00:30 +0900</pubDate>
      
      <guid>https://dschloe.github.io/aws/01_settings/iam_users/</guid>
      <description>라이브러리 불러오기 기 설치된 라이브러리를 불러오도록 한다. import boto3 print(boto3.__version__) 1.23.5 IAM User 관련 주요 코드 다음 코드는 유저를 생성하는 코드이다. def create_user(username): iam = boto3.client(&amp;#39;iam&amp;#39;) response = iam.create_user(UserName=username) print(response) create_user(&amp;#39;testuser2fromwsl2&amp;#39;) {&#39;User&#39;: {&#39;Path&#39;: &#39;/&#39;, &#39;UserName&#39;: &#39;testuser2fromwsl2&#39;, &#39;UserId&#39;: &#39;AIDAVRRRQ3HFXFQPOOY7Q&#39;, &#39;Arn&#39;: &#39;arn:aws:iam::381282212299:user/testuser2fromwsl2&#39;, &#39;CreateDate&#39;: datetime.datetime(2022, 5, 24, 5, 30, 6, tzinfo=tzutc())}, &#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;d5fa242b-9aa9-4ad9-a75a-ed23e041d4ba&#39;, &#39;HTTPStatusCode&#39;: 200, &#39;HTTPHeaders&#39;: {&#39;x-amzn-requestid&#39;: &#39;d5fa242b-9aa9-4ad9-a75a-ed23e041d4ba&#39;, &#39;content-type&#39;: &#39;text/xml&#39;, &#39;content-length&#39;: &#39;495&#39;, &#39;date&#39;: &#39;Tue, 24 May 2022 05:30:05 GMT&#39;}, &#39;RetryAttempts&#39;: 0}} 이번에는 모든 사용자를 가져오는 코드를 작성한다.</description>
    </item>
    
    <item>
      <title>AWS 개발환경 설정 - WSL2 &amp; S3 &amp; RDS</title>
      <link>https://dschloe.github.io/aws/01_settings/settings/</link>
      <pubDate>Thu, 26 May 2022 10:13:30 +0900</pubDate>
      
      <guid>https://dschloe.github.io/aws/01_settings/settings/</guid>
      <description>개요 윈도우 WSL2에서 AWS 개발을 위한 기본 개발환경 설정을 진행한다. WSL2 설치 WSL2 설치 및 사용법은 다음 링크를 통해서 확인한다. (여기서 설치법은 다루지 않는다!) URL : https://www.lainyzine.com/ko/article/how-to-install-wsl2-and-use-linux-on-windows-10/ Restart WSL2 WSL2 처음 작업할 때, 실행한다. exec $SHELL WSL2 주요 필수 패키지 설치 Python 3.8 버전을 설치한다. sudo apt update sudo apt install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa sudo apt install python3.7 Python 버전은 다음과 같다. $ python3 --version Python 3.8.10 만약 Python 버전 변경이 안되면 전체 삭제하고 진행한다.</description>
    </item>
    
    <item>
      <title>Django with Elastic Beanstalk - Settings</title>
      <link>https://dschloe.github.io/aws/03_elastic_beanstalk/ch01_eb_django_settings/</link>
      <pubDate>Sun, 22 May 2022 10:13:30 +0900</pubDate>
      
      <guid>https://dschloe.github.io/aws/03_elastic_beanstalk/ch01_eb_django_settings/</guid>
      <description>한줄 요약 생각보다 쉽지 않기 때문에 Windows로 하기 보다는 WSL2로 하는 것을 권한다. 이 부분은 추후 업데이트 할 예정이다. Windows에 EB CLI 설치 공식 문서 : https://docs.aws.amazon.com/ko_kr/elasticbeanstalk/latest/dg/eb-cli3-install-windows.html
우선 기존 아나콘다로 파이썬 환경이 구축되어 있다면, 아나콘다를 삭제하고 진행한다.
3.8 버전 이상으로 하면, 배포 시 생각보다 잘 되지 않을 가능성이 크다. 필자는 Python 버전도 3.7로 재 설치 했다.
C:\WINDOWS\system32&amp;gt;python --version Python 3.7.4 C:\WINDOWS\system32&amp;gt;pip --version pip 19.0.3 from c:\users\human\appdata\local\programs\python\python37-32\lib\site-packages\pip (python 3.7) pip을 이용하여 EB CLI를 설치한다.</description>
    </item>
    
    <item>
      <title>(AWS Project) BigData with Hadoop 05 - Hive Script 연습 예제</title>
      <link>https://dschloe.github.io/aws/02_bigdataplatform/step_5_run_hive_script/</link>
      <pubDate>Mon, 08 Jun 2020 15:13:30 +0900</pubDate>
      
      <guid>https://dschloe.github.io/aws/02_bigdataplatform/step_5_run_hive_script/</guid>
      <description>I. Getting Started 처음 이 페이지를 방문했다면, 반드시 사전작업을 완료하기를 바란다. (AWS Project) BigData with Hadoop 02 - 사전작업 (AWS Project) BigData with Hadoop 03 - Amazon EMR Cluster 시작 (AWS Project) BigData with Hadoop 04 - Allow SSH Access II. What to do now Hive Script를 제출하는 방법에 대해 준비하였다. 를러스터를 생성할 때 단계를 지정하거나 마스터 노드에 연결하고 로컬 파일 시스템에서 스크립트를 생성하고 명렁어를 사용하여 실행할 수 있다. III.</description>
    </item>
    
    <item>
      <title>(AWS Project) BigData with Hadoop 04 - Allow SSH Access</title>
      <link>https://dschloe.github.io/aws/02_bigdataplatform/step_4_allow_ssh_access/</link>
      <pubDate>Sat, 06 Jun 2020 20:13:30 +0900</pubDate>
      
      <guid>https://dschloe.github.io/aws/02_bigdataplatform/step_4_allow_ssh_access/</guid>
      <description>I. Getting Started 처음 이 페이지를 방문했다면, 반드시 사전작업을 완료하기를 바란다. (AWS Project) BigData with Hadoop 02 - 사전작업 (AWS Project) BigData with Hadoop 03 - Amazon EMR Cluster 시작 II. What to do now Client에서 SSH를 통해 클러스터에 접근하는 방법에 대해 다룬다. (1) Warning 보안 그룹은 클러스터에 대한 인바운드 및 아웃바운드 트래픽을 제어하는 가상 방화벽 역할을 한다. 첫 번째 클러스터를 생성하면 Amazon EMR은 마스터 인스턴스, ElasticMapReduce-master와 연결된 기본 Amazon EMR 관리 Security Group 및 핵심 노드 및 태스크 노드와 연결된 Security Group ElasticMapReduce-slave를 생성한다.</description>
    </item>
    
    <item>
      <title>(AWS Project) BigData with Hadoop 03 - Amazon EMR Cluster 시작</title>
      <link>https://dschloe.github.io/aws/02_bigdataplatform/step_3_launch_emr_cluster/</link>
      <pubDate>Wed, 03 Jun 2020 17:13:30 +0900</pubDate>
      
      <guid>https://dschloe.github.io/aws/02_bigdataplatform/step_3_launch_emr_cluster/</guid>
      <description>I. Getting Started 처음 이 페이지를 방문했다면, 반드시 사전작업을 완료하기를 바란다. (AWS Project) BigData with Hadoop 02 - 사전작업 II. What to do now 이번 포스트에서는 비교적 간단하게 빅데이터 클러스터를 시작하는 과정을 진행한다. 막상 해보면 어려운 것은 아니지만, 언제나 그렇듯이 처음 할 때는 늘 시행착오를 겪게 마련이다. Amazon EMR console창에 있는 Quick Options을 사용한다. Quick Options에 있는 다양한 절차들에 대해 확인이 필요하면 Summary of Quick Options에서 확인해본다. III. Sample Cluster 시작 먼저 AWS에 있는 AWS Management Console을 클릭하여 실행하도록 한다.</description>
    </item>
    
    <item>
      <title>(AWS Project) BigData with Hadoop 02 - 사전작업</title>
      <link>https://dschloe.github.io/aws/02_bigdataplatform/step_2_setup/</link>
      <pubDate>Tue, 02 Jun 2020 16:13:30 +0900</pubDate>
      
      <guid>https://dschloe.github.io/aws/02_bigdataplatform/step_2_setup/</guid>
      <description>I. Amazon S3 Bucket 생성 주요 요건 Hive 쿼리의 출력 데이터를 저장할 Amazon S3 버킷과 폴더를 지정한다. 자습서에서는 default log location을 사용하지만, 원하는 경우에는 custom location을 지정할 수 있다. 하둡의 요구사항 중, bucket과 folder names 다음과 같은 규칙을 적용한다. letters, numbers, periods(.), and hyphens(-) 등을 입력한다. 마지막 글자는 숫자로 끝맺음을 하지 않는다. 이러한 요구 사항을 충족하는 폴더에 이미 액세스할 수 있는 경우 이 튜토리얼에 해당 폴더를 사용하십시오. 출력 폴더는 비어 있어야 한다.</description>
    </item>
    
    <item>
      <title>(AWS Project) BigData with Hadoop 01 - Overview</title>
      <link>https://dschloe.github.io/aws/02_bigdataplatform/step_1_overview/</link>
      <pubDate>Tue, 02 Jun 2020 10:13:30 +0900</pubDate>
      
      <guid>https://dschloe.github.io/aws/02_bigdataplatform/step_1_overview/</guid>
      <description>I. Overview Amazon EMR은 Apache 하둡과 Spark를 쉽고 빠르며 비용 효율적으로 실행하여 대량의 데이터를 처리할 수 있게 해주는 관리형 서비스입니다. Amazon EMR은 Presto, Hive, Pig, HBase 등과 같은 강력하고 입증된 하둡 도구를 지원한다. 이 프로젝트에서는 모든 기능이 작동하는 하둡 클러스터를 배포하여 몇 분 만에 로그 데이터를 분석할 준비를 갖추게 된다. 먼저 Amazon EMR 클러스터를 시작한 다음, HiveQL 스크립트를 사용하여 Amazon S3 버킷에 저장된 샘플 로그 데이터를 처리한다. HiveQL은 데이터 웨어하우징과 분석을 위한 SQL 유사 스크립트 언어이다.</description>
    </item>
    
    <item>
      <title>AWS - 회원가입 및 주요 서비스 간략 소개</title>
      <link>https://dschloe.github.io/aws/01_settings/register/</link>
      <pubDate>Sun, 31 May 2020 17:13:30 +0900</pubDate>
      
      <guid>https://dschloe.github.io/aws/01_settings/register/</guid>
      <description>I. Overview 클라우드 서비스로써, AWS는 모든 IT 개발자에게 필수 Skill set과 같다. 데이터 분석가, 싸이언티스트에게도 AWS는 반드시 알아야 할 서비스이다. 매우 유명한 서비스이기에 자세한 내용은 다루지 않는다. 다만, 데이터 분석가라면 알아두어야 할 필수 서비스만 간단히 다루도록 할 예정이다. 간단하게 회원가입을 진행하도록 하자. II. Registration Step 1. 웹사이트 접속 및 계정 생성 웹사이트: https://aws.amazon.com/ko/ 우측 상단의 [AWS 계정 생성]을 클릭한다. Step 2. [계정 생성] 페이지에서 이메일 주소, 암호, AWS 계정 이름을 입력하고 [계속] 버튼을 클릭한다.</description>
    </item>
    
  </channel>
</rss>
